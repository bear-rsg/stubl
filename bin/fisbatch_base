#!/bin/bash
# -*- coding: utf-8 -*-
# Author Andrew Edmondson (University of Birmingham)
# June 2017
#
# Based on:
#
# Author: PÃ¤r Andersson (National Supercomputer Centre, Sweden)
# Version: 0.3 2007-07-30
#
# 2011-06-23: Joerg Bornschein <bornschein@fias.uni-frankfurt.de>
#   Make this script find its own path
#   https://github.com/jbornschein/srun.x11/blob/master/srun.x11
#
# 2014-06-26: L. Shawn Matott <lsmatott@buffalo.edu>
#   fisbatch is based on srun.x11 with extensions to handle
#   centers that have multiple clusters and partitions. It also,
#   has additional logic to detect and report downtimes rather
#   than leave users waiting on resources that are not available
#   and likely won't be available for some time.
#
# This will submit a batch script that starts screen on a node.
# Then ssh is used to connect to the node and attach the screen.
# The result is very similar to an interactive shell in PBS
# (qsub -I)
#
#  FISBATCH = Friendly Interactive SBATCH
#


if [ "$1" == "--help" ]; then
  echo " "
  echo "========================================="
  echo "fisbatch                                 "
  echo " "
  echo "   A Friendly Interactive SBATCH command."
  echo " "
  echo "   Usage:                                "
  echo "      fisbatch [sbatch directives]       "
  echo "========================================="
  echo " "
  exit
fi

function log () {
    echo -e "\e[92m[info] $@\e[0m"
}
function error () {
    echo -e "\e[91m[error] $@\e[0m"
}

if [[ -n ${FISBATCH_DEBUG} ]] && [[ ${FISBATCH_DEBUG} -eq 1 ]]; then
    log "Debug mode"
    set -x
fi

if [[ -z ${FISBATCH_LAUNCHER_SCRIPT} ]]; then
    error "FISBATCH_LAUNCHER_SCRIPT is not defined"
    exit 88
fi
if [[ -z ${FISBATCH_ATTACH_SCRIPT} ]]; then
    error "FISBATCH_ATTACH_SCRIPT is not defined"
    exit 88
fi
if [[ -z ${FISBATCH_CHECK_REGEX} ]]; then
    error "FISBATCH_CHECK_REGEX is not defined"
    exit 88
fi

# determine STUBL install location
# Copied from Apache Ant:
# https://git-wip-us.apache.org/repos/asf?p=ant.git;a=blob;f=src/script/ant;h=b5ed5be6a8fe3a08d26dea53ea0fb3f5fab45e3f
if [ -z "$STUBL_HOME" -o ! -d "$STUBL_HOME" ] ; then
  ## resolve links - $0 may be a link to stubl's home
  PRG="$0"
  progname=`basename "$0"`

  # need this for relative symlinks
  while [ -h "$PRG" ] ; do
    ls=`ls -ld "$PRG"`
    link=`expr "$ls" : '.*-> \(.*\)$'`
    if expr "$link" : '/.*' > /dev/null; then
    PRG="$link"
    else
    PRG=`dirname "$PRG"`"/$link"
    fi
  done

  STUBL_HOME=`dirname "$PRG"`/..

  # make it fully qualified
  STUBL_HOME=`cd "$STUBL_HOME" > /dev/null && pwd`
fi

# setup STUBL environment
. $STUBL_HOME/conf/stubl 

# Location of helper scripts
MYDIR=$STUBL_HOME/bin/fisbatch_helpers

cluster=`echo $@ | tr ' ' '\n' | grep "\-\-clusters=" | cut -d'=' -f2`
if [ "$cluster" == "" ]; then
  cluster=$STUBL_DEFAULT_CLUSTER
fi

# Submit the job and get the job id
MyExport=SLURM_CPUS_PER_TASK,SLURM_JOB_NAME,SLURM_NTASKS_PER_NODE,SLURM_PRIO_PROCESS,SLURM_SUBMIT_DIR,SLURM_SUBMIT_HOST
SBATCH_OUT="$(sbatch --job-name=FISBATCH $@ ${MYDIR}/${FISBATCH_LAUNCHER_SCRIPT} 2>&1)"
JOB=$(echo ${SBATCH_OUT} | egrep -o -e "\b[0-9]+")
JOB=$(echo $JOB | awk '{ printf("%d", $1 + 0); }')

if [[ -z ${JOB} ]] || [[ ${JOB} -eq 0 ]]; then
    error "Couldn't find job id"
    error "Slurm said: ${SBATCH_OUT}"
    exit 2
fi

# Make sure the job is always canceled
function cleanup () {
    log 'Cancelling job'
    $STUBL_SCANCEL $JOB 2>/dev/null
}

trap cleanup SIGINT SIGTERM EXIT

log "Waiting for JOBID $JOB to start"
zero_count=0
while true; do
    sleep 1s

    # Check job status
    STATUS=$(squeue -j $JOB -t PD,R -h -o %t | grep -v "^CLUSTER")
    if [[ "$STATUS" = "R" ]];then
        # Job is running, break the while loop
        echo
            log "Job ${JOB} is running"
        break
    elif [[ "${STATUS}" = "PD" ]]; then
        echo -n "."
    elif [[ -z $STATUS ]] ; then
        echo -n "-"
        ((zero_count++))
        if [[ ${zero_count} -gt 5 ]]; then
            error "\nJob seems to have failed"
            test -e slurm-${JOB}.out && error "See slurm-${JOB}.out for details"
            exit 2
        fi
    else
        error "Job is not Running or Pending. Aborting"
        test -e slurm-${JOB}.out && error "See slurm-${JOB}.out for details"
        exit 3
    fi

    sleep 1s
done

# Determine the head node in the job:
HNODE=""
usr=`whoami`
NODE=$(scontrol show jobid=${JOB} | egrep -o "NodeList=([a-z0-9]+)" | awk -F '=' '{print $2}')

log "Job is running on ${NODE}"

for i in {1..1000}; do
    for i in $NODE; do
       screenTest=$(ssh $i "ps -u $usr -ef | grep ${FISBATCH_CHECK_REGEX} | wc -l" 2>/dev/null)
       if [[ "$screenTest" != "0" ]]; then
          HNODE=$i
          break
       fi
    done

    if [[ -n "$HNODE" ]]; then
        break
    fi
    sleep 1s
done

if [ "$HNODE" == "" ]; then
    error "Couldn't identify the head node - job not running on any node!"
    exit
fi

for i in {1..10}; do
    log "Connecting to head node ($HNODE)"
    # a brief pause is needed?
    sleep 1s
    # SSH to the node and attach the screen
    ssh -X -t $HNODE ${MYDIR}/${FISBATCH_ATTACH_SCRIPT} slurm$JOB && break
done


# The trap will now cancel the job before exiting.
test -e slurm-${JOB}.out && log "See slurm-${JOB}.out for details"
