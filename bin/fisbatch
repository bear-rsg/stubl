#!/bin/bash
# -*- coding: utf-8 -*-
# Author: PÃ¤r Andersson (National Supercomputer Centre, Sweden)
# Version: 0.3 2007-07-30
#
# 2011-06-23: Joerg Bornschein <bornschein@fias.uni-frankfurt.de>
#   Make this script find its own path
#   https://github.com/jbornschein/srun.x11/blob/master/srun.x11
#
# 2014-06-26: L. Shawn Matott <lsmatott@buffalo.edu>
#   fisbatch is based on srun.x11 with extensions to handle
#   centers that have multiple clusters and partitions. It also,
#   has additional logic to detect and report downtimes rather
#   than leave users waiting on resources that are not available
#   and likely won't be available for some time.
#
# This will submit a batch script that starts screen on a node.
# Then ssh is used to connect to the node and attach the screen.
# The result is very similar to an interactive shell in PBS
# (qsub -I)
#
#  FISBATCH = Friendly Interactive SBATCH
#

if [ "$1" == "--help" ]; then
  echo " "
  echo "========================================="
  echo "fisbatch                                 "
  echo " "
  echo "   A Friendly Interactive SBATCH command."
  echo " "
  echo "   Usage:                                "
  echo "      fisbatch [sbatch directives]       "
  echo "========================================="
  echo " "
  exit
fi

function log () {
    echo -e "\e[92m[info] $@\e[0m"
}
function error () {
    echo -e "\e[91m[error] $@\e[0m"
}

# determine STUBL install location
# Copied from Apache Ant:
# https://git-wip-us.apache.org/repos/asf?p=ant.git;a=blob;f=src/script/ant;h=b5ed5be6a8fe3a08d26dea53ea0fb3f5fab45e3f
if [ -z "$STUBL_HOME" -o ! -d "$STUBL_HOME" ] ; then
  ## resolve links - $0 may be a link to stubl's home
  PRG="$0"
  progname=`basename "$0"`

  # need this for relative symlinks
  while [ -h "$PRG" ] ; do
    ls=`ls -ld "$PRG"`
    link=`expr "$ls" : '.*-> \(.*\)$'`
    if expr "$link" : '/.*' > /dev/null; then
    PRG="$link"
    else
    PRG=`dirname "$PRG"`"/$link"
    fi
  done

  STUBL_HOME=`dirname "$PRG"`/..

  # make it fully qualified
  STUBL_HOME=`cd "$STUBL_HOME" > /dev/null && pwd`
fi

# setup STUBL environment
. $STUBL_HOME/conf/stubl 

# Location of helper scripts
MYDIR=$STUBL_HOME/bin/fisbatch_helpers

# Batch Script that starts SCREEN
BS=$MYDIR/_interactive
# Interactive screen script
IS=$MYDIR/_interactive_screen

cluster=`echo $@ | tr ' ' '\n' | grep "\-\-clusters=" | cut -d'=' -f2`
if [ "$cluster" == "" ]; then
  cluster=$STUBL_DEFAULT_CLUSTER
fi

# Submit the job and get the job id
MyExport=SLURM_CPUS_PER_TASK,SLURM_JOB_NAME,SLURM_NTASKS_PER_NODE,SLURM_PRIO_PROCESS,SLURM_SUBMIT_DIR,SLURM_SUBMIT_HOST
SBATCH_OUT="$(sbatch --job-name=FISBATCH $@ $BS 2>&1)"
JOB=$(echo ${SBATCH_OUT} | egrep -o -e "\b[0-9]+")
JOB=$(echo $JOB | awk '{ printf("%d", $1 + 0); }')

if [[ -z ${JOB} ]] || [[ ${JOB} -eq 0 ]]; then
    error "Couldn't find job id"
    error "Slurm said: ${SBATCH_OUT}"
    exit 2
fi

# Make sure the job is always canceled
trap "{ log 'Cancelling job'; $STUBL_SCANCEL $JOB 2>/dev/null; exit; }" SIGINT SIGTERM EXIT

log "Waiting for JOBID $JOB to start"
while true; do
    sleep 1s

    # Check job status
    STATUS=$(squeue -j $JOB -t PD,R -h -o %t | grep -v "^CLUSTER")

    if [[ "$STATUS" = "R" ]];then
	# Job is running, break the while loop
        log "Job ${JOB} is running"
	break
    elif [[ -z $STATUS ]] ; then
        error "\nJob seems to have failed"
        test -e slurm-${JOB}.out && error "See slurm-${JOB}.out for details"
        exit 2
    elif [[ "$STATUS" != "PD" ]];then
	error "Job is not Running or Pending. Aborting"
	exit 3
    fi

    sleep 1s
    echo -n "."
done

# Determine the head node in the job:
HNODE=""
usr=`whoami`
NODE=$(scontrol show jobid=${JOB} | egrep -o "NodeList=([a-z0-9]+)" | awk -F '=' '{print $2}')

log "Job is running on ${NODE}"

for i in $NODE; do
   screenTest=$(ssh $i "ps -ef | grep $usr | grep \[S\]CREEN | wc -l" 2>/dev/null)
   if [[ "$screenTest" != "0" ]]; then
      HNODE=$i
      break
   fi
done

if [ "$HNODE" == "" ]; then
  error "Couldn't identify the head node - SCREEN not running on any node!"
  test -e slurm-${JOB}.out && error "See slurm-${JOB}.out for details"
  exit
fi

log "Connecting to head node ($HNODE)"
# a brief pause is needed?
sleep 1s

# SSH to the node and attach the screen
ssh -X -t $HNODE $IS slurm$JOB

# The trap will now cancel the job before exiting.
